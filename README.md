# Scalable Address Matching Pipeline

## Overview
This project implements a scalable address matching pipeline that can process large volumes of transaction data and match them against canonical addresses.

## Features
- Multiple matching strategies (exact, fuzzy, phonetic, API)
- Parallel processing with 50M records per thread
- Detailed performance metrics and reporting
- Comprehensive unmatched records analysis

## Setup
1. Install dependencies:
```bash
pip install -r requirements.txt
```

2. Set up database:
```bash
psql -U your_user -d your_database -f schema.sql
```

3. Run with Docker:
```bash
docker build -t address-matcher .
docker run -v $(pwd)/data:/app/data address-matcher
```

## Data Processing Pipeline

### 1. Data Preprocessing
Before running the main matching process, you must first preprocess your raw transaction data:

1. Place your raw transaction data in `data/raw/transactions_2_11211.csv`
   - Required columns: `id`, `address_line_1`, `address_line_2`

2. Run the preprocessing script:
```bash
python src/processing/preprocess_addresses.py
```

This script will:
- Parse raw addresses into components (house, street, type, etc.)
- Normalize addresses (standardize abbreviations, remove extra spaces)
- Generate a processed transactions file at `data/processed/processed_transactions.csv`

### 2. Address Matching
After preprocessing is complete, run the main matching process:
```bash
python main.py
```

## Data Flow Details

### Preprocessing Steps
1. **Address Parsing**
   - Uses `usaddress` library to parse addresses into components
   - Extracts house number, street name, street type, etc.
   - Handles apartment/unit information

2. **Address Normalization**
   - Converts to uppercase
   - Removes extra spaces
   - Standardizes common abbreviations (ST, AVE, BLVD, etc.)
   - Removes 'UNIT' prefix while preserving unit numbers

3. **Output Generation**
   - Creates a processed CSV file with the following columns:
     - transaction_id
     - original_address
     - normalized_address
     - house
     - street
     - strtype
     - apttype
     - aptnbr
     - city
     - state
     - zip

### Required File Structure
```
data/
├── raw/
│   ├── transactions_2_11211.csv    # Raw transaction data
│   └── 11211 Addresses.csv         # Canonical addresses
└── processed/
    └── processed_transactions.csv   # Generated by preprocessing
```

## Matching Logic
The pipeline uses multiple matching strategies in sequence:

1. **Exact Matching**: Direct comparison of normalized addresses
2. **Fuzzy Matching**: Levenshtein distance with prefix blocking
3. **Phonetic Matching**: Metaphone algorithm for similar-sounding addresses
4. **API Validation**: External address validation service as fallback

## Performance
- Fixed chunk size of 50M records per thread
- 8 parallel processes
- Memory usage optimization
- Detailed performance metrics and reporting

## Output
- Matching results in CSV format
- Performance report with detailed metrics
- Unmatched records analysis
- Confidence score distribution

## Development
- Use `src/processing/address_parser.py` for address parsing
- Use `src/processing/preprocess_addresses.py` for data preprocessing
- Run tests using `tests/test_performance.py`

## Project Structure
```
.
├── config/ # Configuration files
│ └── schema.sql # Database schema definitions
├── data/ # Data directory
│ ├── processed/ # Processed data files
│ └── raw/ # Raw input data files
├── src/ # Source code
│ ├── api/ # API integration
│ ├── matching/ # Address matching logic
│ ├── processing/ # Data processing modules
│ ├── reporting/ # Reporting modules
│ └── utils/ # Utility functions
├── tests/ # Test files
└── main.py # Main application entry point
```

## Performance Results
### Large Scale Test (200M Records)

```
Processing Transactions: 100%|█| 200M/200M [00:14<00:00] 13.7Mrecords/s
Performance Summary:
Total Runtime: 0.3 minutes
Peak Memory: 0.1 GB
```

### Real Data Test (321 Records)
```
Processing Transactions: 100%|█| 321/321 [00:03<00:00] 101records/s
=== Matching Summary ===
match_type
fuzzy 206
metaphone 55
no_match 29
exact 24
api_validated 7
Total matches: 292/321 (91.0%)
```
=== Confidence Score Distribution ===
Scores 0.0-0.2: 29 (9.0%)
Scores 0.2-0.4: 0 (0.0%)
Scores 0.4-0.6: 0 (0.0%)
Scores 0.6-0.8: 0 (0.0%)
Scores 0.8-1.0: 260 (81.0%)
=== Performance Metrics ===
Total runtime: 4.08 seconds

### Performance Features
- Processing speed: 13.7M records/second in large scale test
- Memory efficient: Peak usage of 0.1GB for 200M records
- High match rate: 91% on real data
- Low API cost: $0.28 for 200M records
- Parallel processing with 50M records per thread