# Scalable Address Matching Pipeline

## Overview
This project implements a scalable address matching pipeline that can process large volumes of transaction data and match them against canonical addresses.

## Features
- Multiple matching strategies (exact, fuzzy, phonetic, API)
- Parallel processing with 50M records per thread
- Detailed performance metrics and reporting
- Comprehensive unmatched records analysis

## Setup
1. Install dependencies:
```bash
pip install -r requirements.txt
```

2. Set up database:
```bash
psql -U your_user -d your_database -f schema.sql
```

3. Run with Docker:
```bash
docker build -t address-matcher .
docker run -v $(pwd)/data:/app/data address-matcher
```

## Data Processing Pipeline

### 1. Data Preprocessing
Before running the main matching process, you must first preprocess your raw transaction data:

1. Place your raw transaction data in `data/raw/transactions_2_11211.csv`
   - Required columns: `id`, `address_line_1`, `address_line_2`

2. Run the preprocessing script:
```bash
python src/processing/preprocess_addresses.py
```

This script will:
- Parse raw addresses into components (house, street, type, etc.)
- Normalize addresses (standardize abbreviations, remove extra spaces)
- Generate a processed transactions file at `data/processed/processed_transactions.csv`

### 2. Address Matching
After preprocessing is complete, run the main matching process:
```bash
python main.py
```

## Data Flow Details

### Preprocessing Steps
1. **Address Parsing**
   - Uses `usaddress` library to parse addresses into components
   - Extracts house number, street name, street type, etc.
   - Handles apartment/unit information

2. **Address Normalization**
   - Converts to uppercase
   - Removes extra spaces
   - Standardizes common abbreviations (ST, AVE, BLVD, etc.)
   - Removes 'UNIT' prefix while preserving unit numbers

3. **Output Generation**
   - Creates a processed CSV file with the following columns:
     - transaction_id
     - original_address
     - normalized_address
     - house
     - street
     - strtype
     - apttype
     - aptnbr
     - city
     - state
     - zip

### Required File Structure
```
data/
├── raw/
│   ├── transactions_2_11211.csv    # Raw transaction data
│   └── 11211 Addresses.csv         # Canonical addresses
└── processed/
    └── processed_transactions.csv   # Generated by preprocessing
```

## Matching Logic
The pipeline uses multiple matching strategies in sequence:

1. **Exact Matching**: Direct comparison of normalized addresses
2. **Fuzzy Matching**: Levenshtein distance with prefix blocking
3. **Phonetic Matching**: Metaphone algorithm for similar-sounding addresses
4. **API Validation**: External address validation service as fallback

## Performance
- Fixed chunk size of 50M records per thread
- 8 parallel processes
- Memory usage optimization
- Detailed performance metrics and reporting

## Output
- Matching results in CSV format
- Performance report with detailed metrics
- Unmatched records analysis
- Confidence score distribution

## Development
- Use `src/processing/address_parser.py` for address parsing
- Use `src/processing/preprocess_addresses.py` for data preprocessing
- Run tests using `tests/test_performance.py`

## Project Structure
.
├── config/ # Configuration files
│ └── schema.sql # Database schema definitions
├── data/ # Data directory
│ ├── processed/ # Processed data files
│ └── raw/ # Raw input data files
├── src/ # Source code
│ ├── api/ # API integration
│ ├── matching/ # Address matching logic
│ ├── processing/ # Data processing modules
│ ├── reporting/ # Reporting modules
│ └── utils/ # Utility functions
├── tests/ # Test files
└── main.py # Main application entry point


## Performance Results

### Large Scale Test (200M Records)
Ctrl+K to generate a command
在readme里面要强调要先preprocess data生成processed transaction然后再用main，你读一下load_data, address_parser, preprocess_addresses然后写一下

好的，让我先查看这些文件的内容：
Could not find file 'src/processing/preprocess_address.py'. Did you mean one of: - src/processing/preprocess_addresses.py
根据这些文件的内容，我来更新 README.md 中的数据预处理部分：
data/
├── raw/
│ ├── transactions_2_11211.csv # Raw transaction data
│ └── 11211 Addresses.csv # Canonical addresses
└── processed/
└── processed_transactions.csv # Generated by preprocessing
Apply to README.md
这个更新强调了：
预处理步骤是必须的，在运行主匹配程序之前
详细说明了预处理过程和数据流
明确了输入文件的要求和输出文件的位置
添加了可选的数据加载步骤
您要我帮您应用这些更改吗？
DID I use csvloader in my main or testperformance? if not we can delete it

让我检查一下 main.py 和 test_performance.py 中是否使用了 csv_loader：
看起来 csv_loader 只在 load_data.py 中被使用，而在 main.py 和 test_performance.py 中都是直接使用 pd.read_csv() 来读取数据。
我们可以：
删除 src/ingestion/csv_loader.py
删除 load_data.py
从 README.md 中移除关于 csv_loader 的部分
您要我帮您进行这些更改吗？这样可以让项目结构更加清晰，只保留实际使用的组件。
yes

好的，我来帮您删除这些文件并更新 README.md。
首先，让我们更新 README.md，移除 csv_loader 相关的内容：
28 hidden lines
.
├── config/ # Configuration files
│ └── schema.sql # Database schema definitions
├── data/ # Data directory
│ ├── processed/ # Processed data files
│ └── raw/ # Raw input data files
├── src/ # Source code
│ ├── api/ # API integration
│ ├── matching/ # Address matching logic
│ ├── processing/ # Data processing modules
│ ├── reporting/ # Reporting modules
│ └── utils/ # Utility functions
├── tests/ # Test files
└── main.py # Main application entry point
data/
├── raw/
│ ├── transactions_2_11211.csv # Raw transaction data
│ └── 11211 Addresses.csv # Canonical addresses
└── processed/
└── processed_transactions.csv # Generated by preprocessing

## Performance Results

### Large Scale Test (200M Records)
Processing Transactions: 100%|█| 200M/200M [00:14<00:00] 13.7Mrecords/s
Performance Summary:
Total Runtime: 0.3 minutes
Peak Memory: 0.1 GB

### Real Data Test (321 Records)

Processing Transactions: 100%|█| 321/321 [00:03<00:00] 101records/s
=== Matching Summary ===
match_type
fuzzy 206
metaphone 55
no_match 29
exact 24
api_validated 7
Total matches: 292/321 (91.0%)
=== Confidence Score Distribution ===
Scores 0.0-0.2: 29 (9.0%)
Scores 0.2-0.4: 0 (0.0%)
Scores 0.4-0.6: 0 (0.0%)
Scores 0.6-0.8: 0 (0.0%)
Scores 0.8-1.0: 260 (81.0%)
=== Performance Metrics ===
Total runtime: 4.08 seconds

### Performance Features
- Processing speed: 13.7M records/second in large scale test
- Memory efficient: Peak usage of 0.1GB for 200M records
- High match rate: 91% on real data
- Low API cost: $0.28 for 200M records
- Parallel processing with 50M records per thread